{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“„ Resume Classification & Intelligent Ranking System\n",
    "This notebook covers the end-to-end pipeline for extracting text from resumes, classifying them into job categories using machine learning, and ranking them against a job description using cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Required Libraries\n",
    "Uncomment the following line if you haven't installed the dependencies yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas scikit-learn xgboost python-docx PyPDF2 matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "import docx\n",
    "import PyPDF2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Utility Functions: Extraction & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(file_path):\n",
    "    file_path = Path(file_path)\n",
    "    suffix = file_path.suffix.lower()\n",
    "    text = \"\"\n",
    "    try:\n",
    "        if suffix == \".docx\":\n",
    "            doc = docx.Document(file_path)\n",
    "            text = \" \".join([p.text for p in doc.paragraphs])\n",
    "        elif suffix == \".pdf\":\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() or \"\"\n",
    "        elif suffix == \".txt\":\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                text = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+\\s*', ' ', text)\n",
    "    text = re.sub(r'RT|cc', ' ', text)\n",
    "    text = re.sub(r'#\\S+', ' ', text)\n",
    "    text = re.sub(r'@\\S+', ' ', text)\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
    "    text = re.sub(r'[\\x00-\\x7f]', r' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the path if necessary\n",
    "base_dir = Path(r\"c:\\Users\\gopar\\OneDrive\\Desktop\\Resume\\Resume classification dataset\\Dataset\\Resumes\")\n",
    "data = []\n",
    "\n",
    "categories = {\n",
    "    \"Peoplesoft resumes\": \"Peoplesoft\",\n",
    "    \"SQL Developer Lightning insight\": \"SQL Developer\",\n",
    "    \"workday resumes\": \"Workday\"\n",
    "}\n",
    "\n",
    "for folder in base_dir.iterdir():\n",
    "    if folder.is_dir():\n",
    "        cat_name = categories.get(folder.name, folder.name)\n",
    "        for file in folder.rglob(\"*\"):\n",
    "            if file.suffix.lower() in [\".docx\", \".pdf\", \".txt\"]:\n",
    "                text = extract_text(file)\n",
    "                if text:\n",
    "                    data.append({\"file_name\": file.name, \"text\": text, \"category\": cat_name})\n",
    "    else:\n",
    "        if folder.suffix.lower() in [\".docx\", \".pdf\", \".txt\"]:\n",
    "            text = extract_text(folder)\n",
    "            if text:\n",
    "                label = \"React Developer\" if \"React\" in folder.name else \"Other\"\n",
    "                data.append({\"file_name\": folder.name, \"text\": text, \"category\": label})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "print(f\"Total Resumes Processed: {len(df)}\")\n",
    "print(df['category'].value_counts())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='category', palette='viridis')\n",
    "plt.title(\"Distribution of Resume Categories\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['word_count'], bins=15, kde=True, color='skyblue')\n",
    "plt.title(\"Resume Word Count Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Feature Engineering & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['category_encoded'] = le.fit_transform(df['category'])\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=2000)\n",
    "X = tfidf.fit_transform(df['cleaned_text'])\n",
    "y = df['category_encoded']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model Training & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(probability=True, kernel='linear', random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "}\n",
    "\n",
    "performance_metrics = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    performance_metrics.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1 Score\": f1\n",
    "    })\n",
    "    print(f\"{name} - Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "perf_df = pd.DataFrame(performance_metrics)\n",
    "perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Visualization of Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=perf_df, x='Model', y='Accuracy', palette='coolwarm')\n",
    "plt.title(\"Model Accuracy Comparison\")\n",
    "plt.ylim(0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Saving the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = perf_df.sort_values(by='F1 Score', ascending=False).iloc[0]['Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "with open(\"tfidf.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "df.to_csv(\"processed_resumes.csv\", index=False)\n",
    "print(f\"Best model ({best_model_name}) and artifacts saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. AI Ranking & Shortlisting Simulation\n",
    "Enter a job description to rank existing resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description = \"We are looking for a SQL Developer with experience in database management, query optimization, and PL/SQL.\"\n",
    "\n",
    "# Clean and Vectorize Job Description\n",
    "jd_cleaned = clean_text(job_description)\n",
    "jd_vec = tfidf.transform([jd_cleaned])\n",
    "\n",
    "# Calculate Similarity with all resumes\n",
    "resume_vecs = tfidf.transform(df['cleaned_text'])\n",
    "scores = cosine_similarity(jd_vec, resume_vecs).flatten()\n",
    "\n",
    "df['Match Score'] = (scores * 100).round(2)\n",
    "ranking = df[['file_name', 'category', 'Match Score']].sort_values(by='Match Score', ascending=False)\n",
    "\n",
    "print(\"TOP 5 MATCHES:\")\n",
    "ranking.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
